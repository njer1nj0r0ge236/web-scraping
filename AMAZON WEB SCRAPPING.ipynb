{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMAZON WEB SCRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import smtplib # for sending emails to yourself\n",
    "import time \n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the URL of the webpage to be scrapped\n",
    "url = \"https://www.amazon.com/dp/B0CLZMFGX6?ref=emc_p_m_5_i_atc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user agent for your computer\n",
    "# link= httpbin.org/get\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# pull in all the html\n",
    "page = requests.get(url) #, headers=headers\n",
    "\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\") # .prettify() makes things look better/ better formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   PRETTYGARDEN Womens Ribbed Maxi Bodycon Summer Strapless Tube Y2K Party Club Long Dress\n",
      "                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      4.1\n",
      "                     \n",
      "\n",
      "\n",
      "                       4.1 out of 5 stars\n",
      "                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    102 ratings\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Color:\n",
      "                   \n",
      "\n",
      "                    Lake Blue\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Size:\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      Select\n",
      "                     \n",
      "\n",
      "                      X-Small\n",
      "                     \n",
      "\n",
      "                      Small\n",
      "                     \n",
      "\n",
      "                      Medium\n",
      "                     \n",
      "\n",
      "                      Large\n",
      "                     \n",
      "\n",
      "                      X-Large\n",
      "                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Select\n",
      "                       \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      Update Page\n",
      "                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# specify what we want\n",
    "title = soup2.find(id=\"productTitle\").get_text() # we get the id from the actual website's html...ctrl+shift+i / inspect\n",
    "price = soup2.find(id=\"desktop_unifiedPrice\").get_text()\n",
    "\n",
    "rating = soup2.find(id=\"averageCustomerReviews\").get_text()\n",
    "size = soup2.find(id=\"variation_size_name\").get_text()\n",
    "color = soup2.find(id=\"variation_color_name\").get_text()\n",
    "# size_chart = soup2.find(id=\"sizeChartV2Data_feature_div\").get_text()\n",
    "# shipping = soup2.find(id=\"freeShippingPriceBadging_feature_div\").get_text()\n",
    "\n",
    "print(title)\n",
    "print(price)\n",
    "print(rating)\n",
    "print(color)\n",
    "print(size)\n",
    "# print(size_chart)\n",
    "# print(shipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTYGARDEN Womens Ribbed Maxi Bodycon Summer Strapless Tube Y2K Party Club Long Dress\n"
     ]
    }
   ],
   "source": [
    "# clean \n",
    "# .strip()-remove any junk e.g. unnecessary spaces\n",
    "# [1:]-start from index 1...removes the $ sign in the case\n",
    "# price = price.strip()[1:]  # only run once to avoid errors\n",
    "\n",
    "title = title.strip()\n",
    "rating = rating.strip()\n",
    "size = size.strip()\n",
    "color = color.strip()\n",
    "\n",
    "print(title)\n",
    "#print(price)\n",
    "#print(rating)\n",
    "#print(size)\n",
    "#print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color:\n",
      "                   \n",
      "\n",
      "                    Lake Blue\n"
     ]
    }
   ],
   "source": [
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      Select\n",
      "                     \n",
      "\n",
      "                      X-Small\n",
      "                     \n",
      "\n",
      "                      Small\n",
      "                     \n",
      "\n",
      "                      Medium\n",
      "                     \n",
      "\n",
      "                      Large\n",
      "                     \n",
      "\n",
      "                      X-Large\n",
      "                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        Select\n",
      "                       \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                      Update Page\n"
     ]
    }
   ],
   "source": [
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1\n",
      "                     \n",
      "\n",
      "\n",
      "                       4.1 out of 5 stars\n",
      "                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    102 ratings\n"
     ]
    }
   ],
   "source": [
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-10\n"
     ]
    }
   ],
   "source": [
    "# timestamp to know when you scrapped the data\n",
    "today_date = datetime.date.today()\n",
    "print(today_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .csv to put in the data\n",
    "\n",
    "header = [\"Title\", \"Size\", \"Rating\", \"Date\"]\n",
    "data = [title, size, rating, today_date] # the type of data= list\n",
    "\n",
    "# only run this once or comment it out \n",
    "with open(\"amazon_web_srapping.csv\", \"w\", newline=\"\", encoding=\"UTF8\") as f: # w-right, newline= -ensures there are no spaces between each csv\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)  # Initial insertion of data into the csv\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  PRETTYGARDEN Womens Ribbed Maxi Bodycon Summer...   \n",
      "\n",
      "                                                Size  \\\n",
      "0  Size:\\n                   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
      "\n",
      "                                              Rating        Date  \n",
      "0  4.1\\n                     \\n\\n\\n              ...  2024-06-10  \n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame\n",
    "# for path- go to file, properties,copy location\n",
    "amazon_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\WEB-SCRAPPING\\amazon_web_srapping.csv\")\n",
    "\n",
    "print(amazon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending more data to the csv\n",
    "\n",
    "with open(\"amazon_web_scrapping.csv\", \"a+\", newline=\"\", encoding=\"UTF8\") as f: # w-right, newline= -ensures there are no spaces between each csv\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when tracking a price e.g when you want to catch a sale\n",
    "\n",
    "# define the send_mail\n",
    "def send_mail():\n",
    "    server = smtplib.SMTP_SSL(\"smtp.gmail.com\",465) # connect to server using gmail \n",
    "    server.ehlo()\n",
    "\n",
    "    server.ehlo()\n",
    "    server.login(\"youremail@gmail.com\",\"password\") # log into email\n",
    "\n",
    "    subject = \"The Product you want is below $10! Hurry up and grad it!\"\n",
    "    body = \"Hello, this your chance to get it. Don't miss this chance!\"\n",
    "\n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\n",
    "\n",
    "# send mail\n",
    "    server.sendmail(\n",
    "        \"youremail@gmail.com\",\n",
    "        msg\n",
    "    )\n",
    "\n",
    "# if(price < 10):\n",
    "  #  send_mail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automate the appending\n",
    "##### For a timeseries project\n",
    "##### For products that really flactuate in price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate the appending\n",
    "\n",
    "def check_price():\n",
    "    url = \"https://www.amazon.com/Funyy-Data-Systems-Business-Analyst/dp/B07FNW9FG3/ref-sr_1?dchild=1&keywords=data%2Banalyst%2Bts\"\n",
    "\n",
    "    # headers = \n",
    "    page = requests.get(url) #, headers=headers\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\") # .prettify() makes things look better/ better formatted\n",
    "\n",
    "    title = soup2.find(id='productTitle').get_text() # we get the id from the actual website's html...ctrl+shift+i\n",
    "    price = soup2.find(id='priceblock_ourprice').get_text()\n",
    "\n",
    "    price = price.strip()[1:]  # only run once to avoid errors\n",
    "\n",
    "    title = title.strip()\n",
    "\n",
    "    today_date = datetime.date.today()\n",
    "    print(today_date)\n",
    "\n",
    "    header = [\"Title\", \"Price\", \"Date\"]\n",
    "    data = [title, price, today_date]\n",
    "    \n",
    "    with open(\"name_of_file.csv\", \"a+\", newline=\"\", encoding=\"UTF8\") as f: # w-right, newline= -ensures there are no spaces between each csv\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "\n",
    "    # if(price < 10):\n",
    "    # send_mail()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the above in a timer - time library\n",
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(5) # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame\n",
    "# for path- go to file, properties,copy location\n",
    "amazon_df = pd.read_csv(r\"location\\name_of_file.csv\")\n",
    "print(amazon_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
